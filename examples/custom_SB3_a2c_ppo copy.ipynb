{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gymnasium as gym\n",
        "import gym_mtsim\n",
        "sys.path.append(\"C:/Users/WilliamFetzner/Documents/Trading/\")\n",
        "from gym_mtsim_forked.gym_mtsim.data import FOREX_DATA_PATH_TRAIN, FOREX_DATA_PATH_TEST, FOREX_DATA_PATH\n",
        "from gym_mtsim import OrderType, Timeframe, MtEnv, MtSimulator\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, STATUS_FAIL\n",
        "from stable_baselines3 import A2C, PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "import time\n",
        "import pickle\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# import time\n",
        "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "\n",
        "\n",
        "# def objective(x):\n",
        "#     return {\n",
        "#         'loss': x ** 2,\n",
        "#         'status': STATUS_OK,\n",
        "#         # -- store other results like this\n",
        "#         'eval_time': time.time(),\n",
        "#         'other_stuff': {'type': None, 'value': x},\n",
        "#         # -- attachments are handled differently\n",
        "#         'attachments':\n",
        "#             {'time_module': pickle.dumps(time.time)}\n",
        "#         }\n",
        "# trials = Trials()\n",
        "# best = fmin(objective,\n",
        "#             space=hp.uniform('x', -10, 10),\n",
        "#             algo=tpe.suggest,\n",
        "#             max_evals=100,\n",
        "#             trials=trials)\n",
        "\n",
        "# print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trials.results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# unpack the pickle file and load the data that is in symbols_forex.pkl\n",
        "with open('C:/Users/WilliamFetzner/Documents/Trading/gym_mtsim_forked/gym_mtsim/data/symbols_forex.pkl', 'rb') as f:\n",
        "    symbols = pickle.load(f)\n",
        "# convert symbols to a pd.dataframe\n",
        "# symbols[1]['EURUSD']\n",
        "split = int(len(symbols[1]['EURUSD']) * 0.80)\n",
        "validation_split = int(len(symbols[1]['EURUSD']) * 0.90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the 2 weeks of the symbols[1]['EURUSD'] dataframe by first finding the max date\n",
        "# then subtracting 14 days from that date\n",
        "symbols[1]['EURUSD'].index = pd.to_datetime(symbols[1]['EURUSD'].index)\n",
        "max_date = symbols[1]['EURUSD'].index.max()\n",
        "two_weeks = max_date - pd.DateOffset(days=14)\n",
        "one_week = max_date - pd.DateOffset(days=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_index_slice = symbols[1]['EURUSD'].loc[:two_weeks, :].index\n",
        "validation_index_slice = symbols[1]['EURUSD'].loc[two_weeks:one_week, :].index\n",
        "testing_index_slice = symbols[1]['EURUSD'].loc[one_week:, :].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_index_slice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_train = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH\n",
        ")\n",
        "\n",
        "env_train = MtEnv(\n",
        "    original_simulator=sim_train,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(training_index_slice),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_validation = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH\n",
        ")\n",
        "\n",
        "env_validation = MtEnv(\n",
        "    original_simulator=sim_validation,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(validation_index_slice),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_testing = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH\n",
        ")\n",
        "\n",
        "env_testing = MtEnv(\n",
        "    original_simulator=sim_testing,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(testing_index_slice),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(reward_over_episodes, printing_name):\n",
        "    \"\"\"  Print Reward  \"\"\"\n",
        "\n",
        "    avg_rewards = np.mean(reward_over_episodes)\n",
        "    min_rewards = np.min(reward_over_episodes)\n",
        "    max_rewards = np.max(reward_over_episodes)\n",
        "\n",
        "    print (f'Min. {printing_name}          : {min_rewards:>10.3f}')\n",
        "    print (f'Avg. {printing_name}          : {avg_rewards:>10.3f}')\n",
        "    print (f'Max. {printing_name}          : {max_rewards:>10.3f}')\n",
        "\n",
        "    return min_rewards, avg_rewards, max_rewards\n",
        "\n",
        "\n",
        "# ProgressBarCallback for model.learn()\n",
        "class ProgressBarCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq: int, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        self.progress_bar = tqdm(total=self.model._total_timesteps, desc=\"model.learn()\")\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            self.progress_bar.update(self.check_freq)\n",
        "        return True\n",
        "    \n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        self.progress_bar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "space = {\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, -2), # Learning rate\n",
        "    'gamma': hp.uniform('gamma', 0.97, 0.99), # Discount factor\n",
        "    'ent_coef': hp.loguniform('ent_coef', -5, 0) # Entropy coefficient\n",
        "    # 'learning_timesteps': hp.choice('learning_timesteps', [25, 50, 100, 250, 500]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # create a graph that shows the distribution of values created by 10_000 iterations of 10 ** np.random.uniform(-5, -2)\n",
        "# # and then plot the graph\n",
        "# learning_rate_dist = [10 ** np.random.uniform(-5, 0) for _ in range(10_000)]\n",
        "# sns.histplot(learning_rate_dist, kde=True)\n",
        "# plt.xscale('log')\n",
        "# plt.xlabel('Learning Rate')\n",
        "# plt.title('Learning Rate Distribution')\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def env_walkthrough(env, model, seed, testing_training_env=False, run_count=10):\n",
        "    reward_over_validations = []\n",
        "    orders_over_validations = []\n",
        "\n",
        "    for episode in range(0, run_count):\n",
        "        if testing_training_env:\n",
        "            obs_val = env.reset()\n",
        "        else:\n",
        "            obs_val, info_val = env.reset(seed=seed)\n",
        "\n",
        "        total_reward = 0\n",
        "        done_val = False\n",
        "\n",
        "        while not done_val:\n",
        "            action, _states = model.predict(obs_val)\n",
        "            if testing_training_env:\n",
        "                obs_val, reward_val, done_val, info_val = env.step(action)\n",
        "            else:\n",
        "                obs_val, reward_val, terminated_val, truncated_val, info_val = env.step(action)\n",
        "                done_val = terminated_val or truncated_val\n",
        "\n",
        "            total_reward += reward_val\n",
        "            if done_val:\n",
        "                break\n",
        "        if not testing_training_env:\n",
        "            try:\n",
        "                order_len = len(env.render()['orders'])\n",
        "            except:\n",
        "                order_len = 0\n",
        "\n",
        "        # model_dict[f'model_{episode}'] = model\n",
        "        # model.save(f'models_4_19_24/window_{window_size_param}_entropy_{round(entropy, 4)}/model_{steps_str}_{episode}.pkl')\n",
        "\n",
        "        reward_over_validations.append(total_reward)    \n",
        "        if not testing_training_env:\n",
        "            orders_over_validations.append(order_len)  \n",
        "\n",
        "\n",
        "        # if episode % 1 == 0:\n",
        "        avg_reward = np.mean(reward_over_validations)\n",
        "        if not testing_training_env:\n",
        "            avg_orders = np.mean(orders_over_validations)\n",
        "            print(f'Episode: {episode}, Avg. Reward: {avg_reward:.3f}, # of orders: {avg_orders:.3f}')\n",
        "        else: \n",
        "            print(f'Episode: {episode}, Avg. Reward: {avg_reward:.3f}')\n",
        "\n",
        "        if (avg_reward > 0) and (testing_training_env):\n",
        "            print('model successfully trained!')\n",
        "            break\n",
        "\n",
        "    return reward_over_validations, orders_over_validations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAINING + TEST\n",
        "def train_val_model(model, model_policy, env_tr, env_val, seed, steps_str, window_size_param, lr, gamma_param, entropy, training_attempts=5, training_run_count=10, validating_run_count=10, total_learning_timesteps=10_000):\n",
        "    \"\"\"\n",
        "    Trains and validates a model using the Proximal Policy Optimization (PPO) algorithm.\n",
        "\n",
        "    Args:\n",
        "        model (object): The model to be trained.\n",
        "        model_policy (object): The policy used by the model.\n",
        "        env_tr (object): The training environment.\n",
        "        env_val (object): The validation environment.\n",
        "        seed (int): The random seed for reproducibility.\n",
        "        steps_str (str): A string representing the number of steps.\n",
        "        window_size_param (int): The window size parameter.\n",
        "        lr (float): The learning rate.\n",
        "        gamma_param (float): The gamma parameter.\n",
        "        entropy (float): The entropy coefficient.\n",
        "        total_learning_timesteps (int, optional): The total number of learning timesteps. Defaults to 10,000.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the reward over validations, orders over validations, and the model dictionary.\n",
        "    \"\"\"\n",
        "    # reproduce training and test\n",
        "    print('-' * 80)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    #model_dict = {}\n",
        "    print(f'entropy: {entropy}, learning rate: {lr}')\n",
        "    vec_env = None\n",
        "    # eval_callback = EvalCallback(env_tr, log_path='./logs/', eval_freq=1000)\n",
        "    model = PPO(model_policy, env_tr, verbose=0, ent_coef=entropy, learning_rate=lr)#, gamma=gamma_param, \n",
        "    obs_tr, info_tr = env_tr.reset(seed=seed)\n",
        "    training_success = False\n",
        "    for i in range(training_attempts):\n",
        "        print(f'training model attempt {i}')\n",
        "        model.learn(total_timesteps=total_learning_timesteps)\n",
        "\n",
        "        # checking if the model learned\n",
        "        vec_env = model.get_env()\n",
        "        obs = vec_env.reset()\n",
        "        rewards, _ = env_walkthrough(vec_env, model, seed, testing_training_env=True, run_count=training_run_count)\n",
        "\n",
        "        if np.mean(rewards) > 0:\n",
        "            training_success = True\n",
        "            break\n",
        "    if not training_success:\n",
        "        print('Model failed to learn with those parameters')\n",
        "        return ValueError('Model failed to learn with those parameters')\n",
        "            \n",
        "\n",
        "    return env_walkthrough(env_val, model, seed, run_count=validating_run_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train + Test Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "seed = 2024  # random seed\n",
        "total_num_episodes = 10\n",
        "\n",
        "# print (\"env_name                 :\", env_name)\n",
        "print (\"seed                     :\", seed)\n",
        "\n",
        "# INIT matplotlib\n",
        "plot_settings = {}\n",
        "plot_data = {'x': [i for i in range(1, total_num_episodes + 1)]}\n",
        "\n",
        "# learning_timesteps_list_in_K = [25]#, 50, 100]\n",
        "# learning_timesteps_list_in_K = [50, 250, 500]\n",
        "# learning_timesteps_list_in_K = [500, 1000, 3000, 5000]\n",
        "\n",
        "# RL Algorithms: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
        "\n",
        "timesteps_models_dict = {}\n",
        "def objective(params):\n",
        "    window_size = 10#params['window_size']\n",
        "    learning_timesteps = 50 #params['learning_timesteps']\n",
        "    ent_coef = params['ent_coef']\n",
        "    gamma = params['gamma'] #0.99 #\n",
        "    learning_rate = params['learning_rate']#0.0003#\n",
        "\n",
        "    if learning_rate > 0.05:\n",
        "        print(f'Learning rate too high: {learning_rate}')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    if ent_coef > 0.1:\n",
        "        print(f'Entropy too high: {ent_coef}')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "\n",
        "    total_learning_timesteps = learning_timesteps * 1000\n",
        "    step_key = f'{learning_timesteps}K'\n",
        "    policy_dict = PPO.policy_aliases\n",
        "    policy = policy_dict.get('MultiInputPolicy')\n",
        "    class_name = type(PPO).__qualname__\n",
        "    plot_key = f'{class_name}_rewards_'+step_key\n",
        "    try:\n",
        "        print(f'length of training env time points: {len(env_train.time_points)}, \\\n",
        "              length of validation env time points: {len(env_validation.time_points)}')\n",
        "        rewards, orders = train_val_model(PPO, policy, env_train, env_validation, seed, step_key, window_size, \n",
        "                                                    learning_rate, gamma, ent_coef, total_learning_timesteps=total_learning_timesteps, \n",
        "                                                    training_attempts=2, training_run_count=1, validating_run_count=10)\n",
        "    except:\n",
        "        print(f'''there was an error with those parameters: Window: {window_size}, timesteps: {learning_timesteps}, \\n\n",
        "              ent_coef: {ent_coef}, gamma: {gamma}, learning_rate: {learning_rate}''')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    # timesteps_models_dict[step_key] = models_dict\n",
        "    min_rewards, avg_rewards, max_rewards, = print_stats(rewards, 'Reward')\n",
        "    print_stats(orders, 'Orders')\n",
        "    label = f'Avg. {avg_rewards:>7.2f} : {class_name} - {step_key}'\n",
        "    plot_data[plot_key] = rewards\n",
        "    plot_settings[plot_key] = {'label': label}\n",
        "    params['avg_orders'] = np.mean(orders)     \n",
        "\n",
        "    return {'loss': -avg_rewards, 'status': STATUS_OK, 'eval_time': time.time(), 'parameters': params} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_val_model(PPO, 'MultiInputPolicy', env_train, env_validation, seed, '50K', 10, \n",
        "#                                                     0.0003, 0.9, 0, total_learning_timesteps=50, \n",
        "#                                                     training_attempts=2, training_run_count=1, validating_run_count=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if it is working:\n",
        "parameters = {\n",
        "    'window_size': 10,\n",
        "    'learning_timesteps': 50,\n",
        "    'ent_coef': 0.008841807731982131,\n",
        "    'gamma': 0.9484679718228304,\n",
        "    'learning_rate': 0.021173768344759137\n",
        "}\n",
        "\n",
        "objective(parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PPO('MultiInputPolicy', env_train, verbose=0, ent_coef=parameters['ent_coef']).learn(total_timesteps=25_000) #, learning_rate=parameters['learning_rate'], gamma=parameters['gamma'], ent_coef=parameters['ent_coef']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(0, 35, 7):\n",
        "    training_index_slice = symbols[1]['EURUSD'].loc[:(max_date - pd.DateOffset(days=i+7)), :].index\n",
        "    validation_index_slice = symbols[1]['EURUSD'].loc[(max_date - pd.DateOffset(days=i+7)):(max_date - pd.DateOffset(days=i)), :].index\n",
        "    env_train.time_points = list(training_index_slice)\n",
        "    env_validation.time_points = list(validation_index_slice)\n",
        "    print(f'length of training env time points: {len(env_train.time_points)}, \\\n",
        "          length of validation env time points: {len(env_validation.time_points)}')\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=objective,\n",
        "                space=space,\n",
        "                algo=tpe.suggest,\n",
        "                max_evals=50, # Number of evaluations of the objective function\n",
        "                trials=trials,\n",
        "                trials_save_file=f'hyperopt/trials_4_22_iter_{i}.pkl')\n",
        "\n",
        "    print(\"Best parameters:\", best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trials.results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding hyperparameter results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load in the saved trials data\n",
        "trials = pickle.load(open('hyperopt/trials_04_19.pkl', 'rb'))\n",
        "trials_4_18 = pickle.load(open('hyperopt/trials.pkl', 'rb'))\n",
        "trials_4_19_results = trials.results\n",
        "trials_4_18_results = trials_4_18.results\n",
        "trials_4_19_results.extend(trials_4_18_results)\n",
        "len(trials_4_19_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trials_all_results = trials_4_18_results + trials_4_19_results\n",
        "len(trials_all_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trials_all_results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame()\n",
        "for idx, result in enumerate(trials_all_results):\n",
        "    result['window_size'] = result['parameters']['window_size']\n",
        "    result['learning_rate'] = result['parameters']['learning_rate']\n",
        "    result['ent_coef'] = result['parameters']['ent_coef']\n",
        "    del result['parameters']\n",
        "    new_row = pd.DataFrame(result, index=[idx])\n",
        "    results_df = pd.concat([results_df, new_row], axis=0)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove all the window size values that are not 10\n",
        "results_df = results_df[results_df['window_size'] == 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize the parameters that cause failures in the objective function\n",
        "\n",
        "# create a graph that has learning rate on the x-axis and ent_coef on the y-axis, \n",
        "# then the color of the points is whether the status is ok or fail, green for ok and red for fail\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df['learning_rate'], results_df['ent_coef'], \n",
        "                     c=results_df['status'].apply(lambda x: 'green' if x == 'ok' else 'red'))\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Entropy Coefficient')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "# y lim to 0.2\n",
        "# plt.ylim(0, 0.2)\n",
        "# x lim to 0.05\n",
        "# plt.xlim(0, 0.05)\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=['OK', 'Fail'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize the parameters that cause failures in the objective function\n",
        "\n",
        "# create a graph that has learning rate on the x-axis and ent_coef on the y-axis, \n",
        "# then the color of the points is whether the status is ok or fail, green for ok and red for fail\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df['learning_rate'], results_df['ent_coef'], \n",
        "                     c=results_df['status'].apply(lambda x: 'green' if x == 'ok' else 'red'))\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Entropy Coefficient')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "# y lim to 0.2\n",
        "plt.ylim(0, 0.2)\n",
        "# x lim to 0.05\n",
        "plt.xlim(0, 0.05)\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=['OK', 'Fail'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this showed that window sizes above 10 failed\n",
        "\n",
        "# # what is the count of the different window sizes grouped by status\n",
        "# results_df.groupby(['window_size', 'status']).size()\n",
        "# # Define a dictionary that maps window sizes to marker shapes\n",
        "# marker_dict = {10: '^', 20: 'o', 50: 's'}\n",
        "\n",
        "# # Create a new column in the DataFrame that maps window sizes to marker shapes\n",
        "# results_df['marker'] = results_df['window_size'].map(marker_dict)\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# # Loop over each group of points with the same marker shape\n",
        "# for marker in results_df['marker'].unique():\n",
        "#     subset = results_df[results_df['marker'] == marker]\n",
        "#     scatter = ax.scatter(subset['learning_rate'], subset['ent_coef'], \n",
        "#                          c=subset['status'].apply(lambda x: 'green' if x == 'ok' else 'red'), \n",
        "#                          marker=marker)\n",
        "\n",
        "# ax.set_xlabel('Learning Rate')\n",
        "# ax.set_ylabel('Entropy Coefficient')\n",
        "# ax.set_title('Hyperparameter Optimization')\n",
        "# # y lim to 0.2\n",
        "# plt.ylim(0, 0.2)\n",
        "# # x lim to 0.05\n",
        "# plt.xlim(0, 0.05)\n",
        "# # increase the figure size\n",
        "# fig.set_size_inches(20, 20)\n",
        "# # plt.legend(handles=scatter.legend_elements()[0], labels=['OK', 'Fail'])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# only successes \n",
        "results_df_success = results_df[results_df['status'] == 'ok']\n",
        "results_df_success_negative = results_df_success[results_df_success['loss'] < 0]\n",
        "results_df_success_negative\n",
        "# sort values from least to greatest loss\n",
        "results_df_success_negative.sort_values(by='loss', ascending=True).head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate a 3d plot of the learning rate, ent_coef, and loss\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "x = results_df_success_negative['learning_rate']\n",
        "y = results_df_success_negative['loss']\n",
        "z = results_df_success_negative['ent_coef']\n",
        "\n",
        "ax.scatter(x, y, z, c=z, cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_zlabel('Entropy Coefficient')\n",
        "\n",
        "# increase the plot size\n",
        "fig.set_size_inches(20, 20)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate a 3d plot of the learning rate, ent_coef, and loss\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "results_df_success_negative_low_entropy = results_df_success_negative[results_df_success_negative['ent_coef'] <= 0.2]\n",
        "\n",
        "x = results_df_success_negative_low_entropy['learning_rate']\n",
        "y = results_df_success_negative_low_entropy['loss']\n",
        "z = results_df_success_negative_low_entropy['ent_coef']\n",
        "\n",
        "ax.scatter(x, y, z, c=z, cmap=cm.coolwarm)\n",
        "\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_zlabel('Entropy Coefficient')\n",
        "\n",
        "\n",
        "\n",
        "# increase the plot size\n",
        "fig.set_size_inches(20, 20)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize the parameters that cause failures in the objective function\n",
        "\n",
        "# create a graph that has learning rate on the x-axis and ent_coef on the y-axis, \n",
        "# then the color of the points is whether the status is ok or fail, green for ok and red for fail\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df_success['learning_rate'], results_df_success['ent_coef'], \n",
        "                     c=results_df_success['loss'].apply(lambda x: 'green' if x < 0 else 'red'))\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Entropy Coefficient')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "# plt.legend(handles=scatter.legend_elements()[0], labels=['OK', 'Fail'])\n",
        "plt.ylim(0, 0.1)\n",
        "# increase the plot size\n",
        "fig.set_size_inches(20, 20)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# create a heatmap with learning rate on the x-axis and ent_coef on the y-axis and the color is the loss\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df_success['learning_rate'], results_df_success['ent_coef'], \n",
        "                     c=results_df_success['loss'], cmap='viridis')\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Entropy Coefficient')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "plt.ylim(0, 0.1)\n",
        "# increase the plot size\n",
        "fig.set_size_inches(25, 15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_success_low_entropy = results_df_success[results_df_success['ent_coef'] <= 0.2]\n",
        "# create a heatmap with learning rate on the x-axis and ent_coef on the y-axis and the color is the loss\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df_success_low_entropy['learning_rate'], results_df_success_low_entropy['loss'], \n",
        "                     c=results_df_success_low_entropy['ent_coef'], cmap=cm.coolwarm)\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "# plt.ylim(-50_000, 50_000)\n",
        "# increase the plot size\n",
        "fig.set_size_inches(25, 15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_success_low_entropy = results_df_success[results_df_success['ent_coef'] <= 0.2]\n",
        "# create a heatmap with learning rate on the x-axis and ent_coef on the y-axis and the color is the loss\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df_success_low_entropy['ent_coef'], results_df_success_low_entropy['loss'], \n",
        "                     c=results_df_success_low_entropy['learning_rate'], cmap=cm.coolwarm)\n",
        "ax.set_xlabel('Entropy Coefficient')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "# plt.ylim(-50_000, 50_000)\n",
        "# increase the plot size\n",
        "fig.set_size_inches(25, 15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # create a dataframe of the rewards\n",
        "# rewards_df = pd.DataFrame({'rewards': rewards})\n",
        "# # plot the rewards\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.lineplot(data=rewards_df)\n",
        "# plt.title('Rewards')\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Reward')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # import the models from /models folder\n",
        "# import os\n",
        "# import glob\n",
        "# # get the list of models\n",
        "# model_list = glob.glob('models_4_17_24/*.pkl')\n",
        "# # separate the strings of each model name on _ and get the last element of the string if the string of the model doesn't include 'initial' or 'updated'\n",
        "# model_list_episode_nbr = [model.split('_')[-1] for model in model_list if 'initial' not in model and 'updated' not in model]\n",
        "# model_list_episode_nbr = [int(model_name.split('.')[0]) for model_name in model_list_episode_nbr]\n",
        "# max_episode = max(model_list_episode_nbr)\n",
        "# # test the last set of 10 episodes\n",
        "# init_episode = ((int(max_episode)/10) - 10)*10\n",
        "# # print(max_episode, init_episode)\n",
        "# models = []\n",
        "# # test the last set of 10 episodes from init_episode to max_episode\n",
        "# for nbr in range(int(init_episode), int(max_episode)+10, 10):\n",
        "#     # set up the appropriate time_points for each of the models in the list\n",
        "#     env_train.time_points = list(symbols[1]['EURUSD'].iloc[-int(training_length):-(int(testing_length)-int(nbr)), :].index)# make this -nbr not +nbr next time\n",
        "#     obs_train, info_train = env_train.reset(seed=2024)\n",
        "#     # find the model name that contains the nbr\n",
        "#     model_name = [model for model in model_list if str(nbr) in model][0]\n",
        "#     print(model_name)\n",
        "#     # load the models into a list\n",
        "#     models.append(PPO.load(model_name, env=env_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sim_testing = gym_mtsim.MtSimulator(\n",
        "#     unit='USD',\n",
        "#     balance=200000.,\n",
        "#     leverage=100.,\n",
        "#     stop_out_level=0.2,\n",
        "#     hedge=True,\n",
        "#     symbols_filename=FOREX_DATA_PATH\n",
        "# )\n",
        "\n",
        "# env_testing = MtEnv(\n",
        "#     original_simulator=sim_testing,\n",
        "#     trading_symbols=['EURUSD'],\n",
        "#     window_size = window_size_param,\n",
        "#     time_points=list(testing_index_slice),\n",
        "#     hold_threshold=0.1,\n",
        "#     close_threshold=0.1,\n",
        "#     fee=lambda symbol: {\n",
        "#         # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "#         'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "#         # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "#     }[symbol],\n",
        "#     symbol_max_orders=2,\n",
        "#     multiprocessing_processes=2\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_ppo = PPO.load(f'models_4_17_24\\model_25K_5.pkl', env=env_train)\n",
        "\n",
        "# obs_test, info_test = env_testing.reset(seed=2024)\n",
        "# done_test = False\n",
        "# while not done_test:\n",
        "#     action, _states = model_ppo.predict(obs_test)\n",
        "#     obs_test, reward_test, terminated_test, truncated_test, info_test = env_testing.step(action)\n",
        "#     done_test = terminated_test or truncated_test\n",
        "#     # total_reward += reward_test\n",
        "#     if done_test:\n",
        "#         break\n",
        "# try:\n",
        "#     order_len = len(env_testing.render()['orders'])\n",
        "# except:\n",
        "#     order_len = 0\n",
        "# # print(f\"Episode: {episode}, Reward: {total_reward:.3f}, # orders: {order_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # if model_dict is still a thing\n",
        "# for timestep in timesteps_models_dict.keys():\n",
        "#     models_dict = timesteps_models_dict[timestep]\n",
        "\n",
        "#     for nbr in range(0, 10):\n",
        "#         msg = f\"{'-'*8} Testing Model {nbr} with {timestep} training timesteps {'-'*8}\"\n",
        "#         print(f\"\"\"{msg}\\n{'-'*len(msg)}\"\"\")\n",
        "#         reward_across_episodes = []\n",
        "#         rewards_dict = {}\n",
        "#         model_results_dict = {}\n",
        "#         for episode in range(0, 10):   \n",
        "#             total_reward = 0\n",
        "#             done_test = False\n",
        "#             model_ppo = models_dict[f'model_{nbr}']\n",
        "\n",
        "#             obs_test, info_test = env_train.reset(seed=2024)\n",
        "#             while not done_test:\n",
        "#                 action, _states = model_ppo.predict(obs_test)\n",
        "#                 obs_test, reward_test, terminated_test, truncated_test, info_test = env_train.step(action)\n",
        "#                 done_test = terminated_test or truncated_test\n",
        "#                 total_reward += reward_test\n",
        "#                 if done_test:\n",
        "#                     break\n",
        "#             reward_across_episodes.append(total_reward)\n",
        "#             try:\n",
        "#                 order_len = len(env_train.render()['orders'])\n",
        "#             except:\n",
        "#                 order_len = 0\n",
        "#             print(f\"Episode: {episode}, Reward: {total_reward:.3f}, # orders: {order_len}\")\n",
        "#         print_stats(reward_across_episodes)\n",
        "#         model_results_dict[f'model_{nbr}_{timestep}'] = reward_across_episodes\n",
        "# model_results_df = pd.DataFrame(model_results_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # plot the rewards for each model over episodes\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.lineplot(data=model_results_df)\n",
        "# plt.title('Rewards')\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Reward')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # if model_dict is still a thing\n",
        "# for timestep in ['25K']:\n",
        "#     for nbr in tqdm(range(1, 10)):\n",
        "#         msg = f\"{'-'*8} Testing Model {nbr} with {timestep} training timesteps {'-'*8}\"\n",
        "#         print(f\"\"\"{msg}\\n{'-'*len(msg)}\"\"\")\n",
        "#         reward_across_episodes = []\n",
        "#         number_of_orders_across_episodes = []\n",
        "#         rewards_dict = {}\n",
        "#         model_results_dict = {}\n",
        "#         for episode in tqdm(range(0, 10)):   \n",
        "#             total_reward = 0\n",
        "#             done_test = False\n",
        "#             model_ppo = PPO.load(f'models_4_17_24\\model_{timestep}_{nbr}.pkl', env=env_train)\n",
        "\n",
        "#             obs_test, info_test = env_testing.reset(seed=2024)\n",
        "#             while not done_test:\n",
        "#                 action, _states = model_ppo.predict(obs_test)\n",
        "#                 obs_test, reward_test, terminated_test, truncated_test, info_test = env_testing.step(action)\n",
        "#                 done_test = terminated_test or truncated_test\n",
        "#                 total_reward += reward_test\n",
        "#                 if done_test:\n",
        "#                     break\n",
        "#             reward_across_episodes.append(total_reward)\n",
        "#             try:\n",
        "#                 order_len = len(env_testing.render()['orders'])\n",
        "#             except:\n",
        "#                 order_len = 0\n",
        "#             number_of_orders_across_episodes.append(order_len)\n",
        "#             print(f\"Episode: {episode}, Reward: {total_reward:.3f}, # orders: {order_len}\")\n",
        "#         print_stats(reward_across_episodes, 'Reward')\n",
        "#         print_stats(number_of_orders_across_episodes, 'Orders')\n",
        "#         model_results_dict[f'model_{nbr}_{timestep}'] = reward_across_episodes\n",
        "# model_results_df = pd.DataFrame(model_results_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # plot the rewards for each model over episodes\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.lineplot(data=model_results_df)\n",
        "# plt.title('Rewards')\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Reward')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # if the cluster has been restarted\n",
        "# for nbr, time_points_idx in zip(range(0, 10), range(0, 500, 50)):\n",
        "#     # model_ppo.learn(total_timesteps=25000, callback=ProgressBarCallback(100))\n",
        "#     env_train = MtEnv(\n",
        "#         original_simulator=sim_train,\n",
        "#         trading_symbols=['EURUSD'],\n",
        "#         window_size = window_size_param,\n",
        "#         time_points=list(symbols[1]['EURUSD'].iloc[-int(training_length):-(int(testing_length)-int(time_points_idx)), :].index),\n",
        "#         hold_threshold=0.5,\n",
        "#         close_threshold=0.5,\n",
        "#         fee=lambda symbol: {\n",
        "#             # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "#             'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "#             # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "#         }[symbol],\n",
        "#         symbol_max_orders=2,\n",
        "#         multiprocessing_processes=2\n",
        "#     )\n",
        "#     # obs_train, info_train = env_train.reset(seed=2024)\n",
        "#     total_reward = 0\n",
        "#     done_test = False\n",
        "#     model_ppo = PPO.load(f'models\\model_{nbr}.pkl', env=env_train)\n",
        "\n",
        "#     env_testing = MtEnv(\n",
        "#         original_simulator=sim_testing,\n",
        "#         trading_symbols=['EURUSD'],\n",
        "#         window_size = window_size_param,\n",
        "#         # time_points=list(testing_index_slice),\n",
        "#         hold_threshold=0.5,\n",
        "#         close_threshold=0.5,\n",
        "#         fee=lambda symbol: {\n",
        "#             # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "#             'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "#             # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "#         }[symbol],\n",
        "#         symbol_max_orders=2,\n",
        "#         multiprocessing_processes=2\n",
        "#     )\n",
        "#     obs_test, info_test = env_testing.reset(seed=2024)\n",
        "#     while not done_test:\n",
        "#         action, _states = model_ppo.predict(obs_test)\n",
        "#         obs_test, reward_test, terminated_test, truncated_test, info_test = env_testing.step(action)\n",
        "#         done_test = terminated_test or truncated_test\n",
        "#         total_reward += reward_test\n",
        "#         if done_test:\n",
        "#             break\n",
        "#     state = env_testing.render()\n",
        "\n",
        "#     print(\n",
        "#         f\"balance: {state['balance']}, equity: {state['equity']}, margin: {state['margin']}\\n\"\n",
        "#         f\"free_margin: {state['free_margin']}, margin_level: {state['margin_level']}\\n\"\n",
        "\n",
        "#     )\n",
        "#     # print(state['orders'].Profit.sum())\n",
        "#     if len(state['orders']) > 0:\n",
        "#         print(state['orders'].Profit.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max_episode = 120\n",
        "# for model_nbr in range(0, int(max_episode)+10, 10):\n",
        "#     print(f'Model: {model_nbr}')\n",
        "#     over_episodes_rewards = []\n",
        "#     over_episodes_balance = []\n",
        "#     obs_training, info_training = env_train.reset(seed=2024)\n",
        "#     for episode in range(0, 10):\n",
        "#         obs_test, info_test = env_testing.reset(seed=2024)\n",
        "        \n",
        "#         # model_ppo.learn(total_timesteps=25000, callback=ProgressBarCallback(100))\n",
        "\n",
        "#         total_reward = 0\n",
        "#         done_test = False\n",
        "#         env_train.time_points = list(symbols[1]['EURUSD'].iloc[-int(training_length):-(int(testing_length)-int(model_nbr)), :].index)\n",
        "#         obs_training, info_training = env_train.reset(seed=2024)\n",
        "#         model_ppo = PPO.load(f'models\\model_{model_nbr}.pkl', env=env_train)\n",
        "\n",
        "#         while not done_test:\n",
        "#             action, _states = model_ppo.predict(obs_test)\n",
        "#             obs_test, reward_test, terminated_test, truncated_test, info_test = env_testing.step(action)\n",
        "#             done_test = terminated_test or truncated_test\n",
        "\n",
        "#             total_reward += reward_test\n",
        "#             if done_test:\n",
        "#                 break\n",
        "#         over_episodes_balance.append(info_test['balance'])\n",
        "#         over_episodes_rewards.append(total_reward)\n",
        "#         print(f'Episode: {episode}, Reward: {total_reward:.3f}, Balance: {info_test[\"balance\"]:.3f}')\n",
        "#     print_stats(over_episodes_rewards)\n",
        "#     print_stats(over_episodes_balance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = pd.DataFrame(plot_data)\n",
        "\n",
        "# sns.set_style('whitegrid')\n",
        "# plt.figure(figsize=(8, 6))\n",
        "\n",
        "# for key in plot_data:\n",
        "#     if key == 'x':\n",
        "#         continue\n",
        "#     label = plot_settings[key]['label']\n",
        "#     line = plt.plot('x', key, data=data, linewidth=1, label=label)\n",
        "\n",
        "# plt.xlabel('episode')\n",
        "# plt.ylabel('reward')\n",
        "# plt.title('Random vs. SB3 Agents')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "p3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
