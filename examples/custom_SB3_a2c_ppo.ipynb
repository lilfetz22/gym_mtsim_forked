{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gymnasium as gym\n",
        "import gym_mtsim\n",
        "sys.path.append(\"C:/Users/WilliamFetzner/Documents/Trading/\")\n",
        "from gym_mtsim_forked.gym_mtsim.data import FOREX_DATA_PATH_TRAIN, FOREX_DATA_PATH_TEST, FOREX_DATA_PATH\n",
        "from gym_mtsim import OrderType, Timeframe, MtEnv, MtSimulator\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, STATUS_FAIL\n",
        "from stable_baselines3 import A2C, PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "import time\n",
        "import torch\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# import time\n",
        "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "\n",
        "\n",
        "# def objective(x):\n",
        "#     return {\n",
        "#         'loss': x ** 2,\n",
        "#         'status': STATUS_OK,\n",
        "#         # -- store other results like this\n",
        "#         'eval_time': time.time(),\n",
        "#         'other_stuff': {'type': None, 'value': x},\n",
        "#         # -- attachments are handled differently\n",
        "#         'attachments':\n",
        "#             {'time_module': pickle.dumps(time.time)}\n",
        "#         }\n",
        "# trials = Trials()\n",
        "# best = fmin(objective,\n",
        "#             space=hp.uniform('x', -10, 10),\n",
        "#             algo=tpe.suggest,\n",
        "#             max_evals=100,\n",
        "#             trials=trials)\n",
        "\n",
        "# print(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trials.results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# unpack the pickle file and load the data that is in symbols_forex.pkl\n",
        "with open('C:/Users/WilliamFetzner/Documents/Trading/gym_mtsim_forked/gym_mtsim/data/symbols_forex.pkl', 'rb') as f:\n",
        "    symbols = pickle.load(f)\n",
        "# convert symbols to a pd.dataframe\n",
        "# symbols[1]['EURUSD']\n",
        "split = int(len(symbols[1]['EURUSD']) * 0.80)\n",
        "validation_split = int(len(symbols[1]['EURUSD']) * 0.90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the 2 weeks of the symbols[1]['EURUSD'] dataframe by first finding the max date\n",
        "# then subtracting 14 days from that date\n",
        "symbols[1]['EURUSD'].index = pd.to_datetime(symbols[1]['EURUSD'].index)\n",
        "max_date = symbols[1]['EURUSD'].index.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find the previous friday before max_date\n",
        "# what is the day of the week of the max_date\n",
        "max_day_of_week = max_date.dayofweek\n",
        "# subtract the day of the week from the max_date to get the previous friday\n",
        "max_friday = max_date - pd.DateOffset(days=max_day_of_week+2)\n",
        "two_weeks = max_friday - pd.DateOffset(days=14)\n",
        "one_week = max_friday - pd.DateOffset(days=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_index_slice = symbols[1]['EURUSD'].loc[:two_weeks, :].index\n",
        "validation_index_slice = symbols[1]['EURUSD'].loc[two_weeks:one_week, :].index\n",
        "testing_index_slice = symbols[1]['EURUSD'].loc[one_week:max_friday, :].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_index_slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testing_index_slice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_train = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_train = MtEnv(\n",
        "    original_simulator=sim_train,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(training_index_slice),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_validation = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH\n",
        ")\n",
        "\n",
        "env_validation = MtEnv(\n",
        "    original_simulator=sim_validation,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(validation_index_slice),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_testing = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH\n",
        ")\n",
        "\n",
        "env_testing = MtEnv(\n",
        "    original_simulator=sim_testing,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(testing_index_slice),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(reward_over_episodes, printing_name):\n",
        "    \"\"\"  Print Reward  \"\"\"\n",
        "\n",
        "    avg_rewards = np.mean(reward_over_episodes)\n",
        "    min_rewards = np.min(reward_over_episodes)\n",
        "    max_rewards = np.max(reward_over_episodes)\n",
        "\n",
        "    print (f'Min. {printing_name}          : {min_rewards:>10.3f}')\n",
        "    print (f'Avg. {printing_name}          : {avg_rewards:>10.3f}')\n",
        "    print (f'Max. {printing_name}          : {max_rewards:>10.3f}')\n",
        "\n",
        "    return min_rewards, avg_rewards, max_rewards\n",
        "\n",
        "def my_profit_calculation(env_orders, stop_loss):\n",
        "        # env_orders = env_testing.render()['orders']\n",
        "        # stop_loss = 0.001\n",
        "\n",
        "        # normalize the Volume with to have a mean of 1\n",
        "        mean_value = env_orders['Volume'].mean()\n",
        "\n",
        "        # Normalize the column to have a mean of 1\n",
        "        env_orders.loc[:, 'Volume_normalized'] = round((env_orders['Volume'] / mean_value), 2)\n",
        "        # add a column for when the difference between the Entry Price and the Exit Price is greater than stop_loss\n",
        "        env_orders.loc[:, 'stoploss_hit'] = np.where((env_orders['Type'].str.strip() == 'Buy') &\n",
        "                                                        ((env_orders['Entry Price'] - env_orders['Exit Price']) > stop_loss),\n",
        "                                                        1, np.where((env_orders['Type'].str.strip() == 'Sell') &\n",
        "                                                                        ((env_orders['Exit Price'] - env_orders['Entry Price']) > stop_loss),\n",
        "                                                                        1, 0))\n",
        "        env_orders.loc[:, 'Exit Price'] = np.where((env_orders['Type'].str.strip() == 'Buy') & (env_orders['stoploss_hit'] == 1),\n",
        "                                                        env_orders['Entry Price'] - stop_loss,\n",
        "                                                        np.where((env_orders['Type'].str.strip() == 'Sell') & (env_orders['stoploss_hit'] == 1),\n",
        "                                                                env_orders['Entry Price'] + stop_loss, env_orders['Exit Price']))\n",
        "        env_orders.loc[:, 'Profit'] = np.where((env_orders['Type'].str.strip() == 'Buy'),\n",
        "                                                        ((env_orders['Exit Price'] - (env_orders['Fee']/2)) - \n",
        "                                                        (env_orders['Entry Price'] + (env_orders['Fee']/2)))\n",
        "                                                                * 100_000 * env_orders['Volume_normalized'], \n",
        "                                                        np.where((env_orders['Type'].str.strip() == 'Sell'),\n",
        "                                                                ((env_orders['Entry Price'] - (env_orders['Fee']/2)) - \n",
        "                                                                (env_orders['Exit Price'] + (env_orders['Fee']/2)))\n",
        "                                                                * 100_000 * env_orders['Volume_normalized'], np.nan))\n",
        "        total_reward = env_orders.loc[:, 'Profit'].sum()\n",
        "        # Calculate Gross Profit\n",
        "        gross_profit = env_orders.loc[env_orders['Profit'] > 0, 'Profit'].sum()\n",
        "\n",
        "        # Calculate Gross Loss\n",
        "        gross_loss = env_orders.loc[env_orders['Profit'] < 0, 'Profit'].abs().sum()\n",
        "\n",
        "        # Calculate Profit Factor\n",
        "        profit_factor = gross_profit / gross_loss if gross_loss != 0 else np.inf\n",
        "\n",
        "        profit_factor = profit_factor - 1\n",
        "\n",
        "        return profit_factor, total_reward\n",
        "\n",
        "# ProgressBarCallback for model.learn()\n",
        "class ProgressBarCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq: int, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        self.progress_bar = tqdm(total=self.model._total_timesteps, desc=\"model.learn()\")\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            self.progress_bar.update(self.check_freq)\n",
        "        return True\n",
        "    \n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        self.progress_bar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "space = {\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, -2), # Learning rate\n",
        "    'gamma': hp.uniform('gamma', 0.97, 0.99), # Discount factor\n",
        "    'ent_coef': hp.loguniform('ent_coef', -5, 0) # Entropy coefficient\n",
        "    # 'learning_timesteps': hp.choice('learning_timesteps', [25, 50, 100, 250, 500]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAINING + TEST\n",
        "def train_val_model(model, model_policy, env_tr, env_val, seed, steps_str, lr, gamma_param, entropy, total_learning_timesteps=10_000):\n",
        "    \"\"\"\n",
        "    Trains and validates a model using the Proximal Policy Optimization (PPO) algorithm.\n",
        "\n",
        "    Args:\n",
        "        model (object): The model to be trained.\n",
        "        model_policy (object): The policy used by the model.\n",
        "        env_tr (object): The training environment.\n",
        "        env_val (object): The validation environment.\n",
        "        seed (int): The random seed for reproducibility.\n",
        "        steps_str (str): A string representing the number of steps.\n",
        "        window_size_param (int): The window size parameter.\n",
        "        lr (float): The learning rate.\n",
        "        gamma_param (float): The gamma parameter.\n",
        "        entropy (float): The entropy coefficient.\n",
        "        total_learning_timesteps (int, optional): The total number of learning timesteps. Defaults to 10,000.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the reward over validations, orders over validations, and the model dictionary.\n",
        "    \"\"\"\n",
        "    # reproduce training and test\n",
        "    print('-' * 80)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    #model_dict = {}\n",
        "    # env_tr.window_size = window_size_param\n",
        "    print(f'entropy: {entropy}, learning rate: {lr}, gamma: {gamma_param}')\n",
        "    # eval_callback = EvalCallback(env_tr, log_path='./logs/', eval_freq=1000)\n",
        "    model = PPO(model_policy, env_tr, verbose=0, ent_coef=entropy, learning_rate=lr)#, gamma=gamma_param, \n",
        "    obs_tr, info_tr = env_tr.reset(seed=seed)\n",
        "    # custom callback for 'progress_bar'\n",
        "    model.learn(total_timesteps=total_learning_timesteps)#, callback=ProgressBarCallback(100))\n",
        "\n",
        "    reward_over_validations = []\n",
        "    orders_over_validations = []\n",
        "    profit_over_validations = []\n",
        "\n",
        "    for episode in range(0, 10):\n",
        "        obs_val, info_val = env_val.reset(seed=seed)\n",
        "\n",
        "        total_reward = 0\n",
        "        done_val = False\n",
        "\n",
        "        while not done_val:\n",
        "            action, _states = model.predict(obs_val)\n",
        "            obs_val, reward_val, terminated_val, truncated_val, info_val = env_val.step(action)\n",
        "            done_val = terminated_val or truncated_val\n",
        "\n",
        "            total_reward += reward_val\n",
        "            if done_val:\n",
        "                break\n",
        "        try:\n",
        "            orders_made_in_episode = env_val.render()['orders']\n",
        "            order_len = len(orders_made_in_episode)\n",
        "            total_reward, total_profit = my_profit_calculation(orders_made_in_episode, 0.001)\n",
        "            \n",
        "        except:\n",
        "            print('There were not any orders produced by the model')\n",
        "            order_len = 0\n",
        "\n",
        "        # model_dict[f'model_{episode}'] = model\n",
        "        # model.save(f'best_hyperparameters/models_4_26_24/model_{episode}.pkl')\n",
        "\n",
        "        reward_over_validations.append(total_reward) \n",
        "        profit_over_validations.append(total_profit)   \n",
        "        orders_over_validations.append(order_len)  \n",
        "\n",
        "\n",
        "        # if episode % 1 == 0:\n",
        "        avg_reward = np.mean(reward_over_validations)\n",
        "        avg_orders = np.mean(orders_over_validations)\n",
        "        avg_profit = np.mean(profit_over_validations)\n",
        "        print(f'Episode: {episode}, Avg. Reward: {avg_reward:.3f}, # of orders: {avg_orders:.3f}, avg Profit: {avg_profit:.3f}')\n",
        "\n",
        "    return reward_over_validations, orders_over_validations#, model_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_list = [.99, np.nan, 1.5]\n",
        "# take the average of test_list and ignore the nan values\n",
        "np.nanmean(test_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "seed = 2024  # random seed\n",
        "total_num_episodes = 10\n",
        "\n",
        "# print (\"env_name                 :\", env_name)\n",
        "print (\"seed                     :\", seed)\n",
        "\n",
        "# INIT matplotlib\n",
        "plot_settings = {}\n",
        "plot_data = {'x': [i for i in range(1, total_num_episodes + 1)]}\n",
        "\n",
        "# learning_timesteps_list_in_K = [25]#, 50, 100]\n",
        "# learning_timesteps_list_in_K = [50, 250, 500]\n",
        "# learning_timesteps_list_in_K = [500, 1000, 3000, 5000]\n",
        "\n",
        "# RL Algorithms: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
        "\n",
        "timesteps_models_dict = {}\n",
        "def objective(params):\n",
        "    learning_timesteps = 50 #params['learning_timesteps']\n",
        "    ent_coef = params['ent_coef']\n",
        "    gamma = params['gamma'] #0.99 #\n",
        "    learning_rate = params['learning_rate']#0.0003#\n",
        "\n",
        "    if learning_rate > 0.08:\n",
        "        print(f'Learning rate too high: {learning_rate}')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    if ent_coef > 0.1:\n",
        "        print(f'Entropy too high: {ent_coef}')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "\n",
        "    total_learning_timesteps = learning_timesteps * 1000\n",
        "    step_key = f'{learning_timesteps}K'\n",
        "    policy_dict = PPO.policy_aliases\n",
        "    policy = policy_dict.get('MultiInputPolicy')\n",
        "    class_name = type(PPO).__qualname__\n",
        "    plot_key = f'{class_name}_rewards_'+step_key\n",
        "    try:\n",
        "        rewards, orders = train_val_model(PPO, policy, env_train, env_validation, seed, step_key,  \n",
        "                                                    learning_rate, gamma, ent_coef, total_learning_timesteps)\n",
        "    except:\n",
        "        print(f'''there was an error with those parameters: timesteps: {learning_timesteps}, \\n\n",
        "              ent_coef: {ent_coef}, gamma: {gamma}, learning_rate: {learning_rate}''')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    # timesteps_models_dict[step_key] = models_dict\n",
        "    min_rewards, avg_rewards, max_rewards, = print_stats(rewards, 'Reward')\n",
        "    print_stats(orders, 'Orders')\n",
        "    label = f'Avg. {avg_rewards:>7.2f} : {class_name} - {step_key}'\n",
        "    plot_data[plot_key] = rewards\n",
        "    plot_settings[plot_key] = {'label': label}\n",
        "    params['avg_orders'] = np.mean(orders)       \n",
        "\n",
        "    return {'loss': -avg_rewards, 'status': STATUS_OK, 'eval_time': time.time(), 'parameters': params} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # # check if it is working:\n",
        "# parameters = {\n",
        "#     # 'window_size': 10,\n",
        "#     # 'learning_timesteps': 25,\n",
        "#     'ent_coef': 0.008841807731982131,\n",
        "#     # 'gamma': 0.9484679718228304,\n",
        "#     'learning_rate': 0.021173768344759137\n",
        "# }\n",
        "# objective(parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# PPO('MultiInputPolicy', env_train, verbose=0, ent_coef=parameters['ent_coef']).learn(total_timesteps=25_000) #, learning_rate=parameters['learning_rate'], gamma=parameters['gamma'], ent_coef=parameters['ent_coef']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### adding in gamma test ####\n",
        "trials = Trials()\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=500, # Number of evaluations of the objective function\n",
        "            trials=trials,\n",
        "            trials_save_file=f'hyperopt/trials_4_26.pkl')\n",
        "\n",
        "print(\"Best parameters:\", best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make a sound when the code is done\n",
        "import winsound\n",
        "frequency = 2500  # Set Frequency To 2500 Hertz\n",
        "duration = 2000  # Set Duration To 1000 ms == 1 second\n",
        "winsound.Beep(frequency, duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### testing whether I need to tune hyperparameters every week\n",
        "# for i in range(0, 35, 7):\n",
        "#     training_index_slice = symbols[1]['EURUSD'].loc[:(max_friday - pd.DateOffset(days=i+7)), :].index\n",
        "#     validation_index_slice = symbols[1]['EURUSD'].loc[(max_friday - pd.DateOffset(days=i+7)):(max_friday - pd.DateOffset(days=i)), :].index\n",
        "#     env_train.time_points = list(training_index_slice)\n",
        "#     env_validation.time_points = list(validation_index_slice)\n",
        "#     trials = Trials()\n",
        "#     best = fmin(fn=objective,\n",
        "#                 space=space,\n",
        "#                 algo=tpe.suggest,\n",
        "#                 max_evals=50, # Number of evaluations of the objective function\n",
        "#                 trials=trials,\n",
        "#                 trials_save_file=f'hyperopt/trials_4_22_iter_{i}.pkl')\n",
        "\n",
        "#     print(\"Best parameters:\", best)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trials = pickle.load(open(f'hyperopt/trials_4_23_gamma_added.pkl', 'rb'))\n",
        "for trial in trials.results:\n",
        "    trial['iteration'] = 7\n",
        "trials_all_results = trials.results\n",
        "print(len(trials_all_results),\n",
        "trials_all_results[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trials = pickle.load(open(f'hyperopt/trials_4_22_iter_0.pkl', 'rb'))\n",
        "for trial in trials.results:\n",
        "    trial['iteration'] = 0\n",
        "    trial['parameters']['gamma'] = 0.99\n",
        "trials.results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(0, 35, 7):\n",
        "    # load in the saved trials data\n",
        "    trials = pickle.load(open(f'hyperopt/trials_4_22_iter_{i}.pkl', 'rb'))\n",
        "    for trial in trials.results:\n",
        "        trial['iteration'] = i\n",
        "        trial['parameters']['gamma'] = 0.99\n",
        "    trials_all_results.extend(trials.results)\n",
        "    # trials_4_19_results.extend(trials_4_18_results)\n",
        "    # len(trials_4_19_results)\n",
        "print(len(trials_all_results), trials_all_results[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trials_all_results[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame()\n",
        "for idx, result in enumerate(trials_all_results):\n",
        "    result['learning_rate'] = result['parameters']['learning_rate']\n",
        "    result['ent_coef'] = result['parameters']['ent_coef']\n",
        "    result['gamma'] = result['parameters']['gamma']\n",
        "    del result['parameters']\n",
        "    new_row = pd.DataFrame(result, index=[idx])\n",
        "    results_df = pd.concat([results_df, new_row], axis=0)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df.loc[:, 'loss_binary'] = np.where(results_df['loss'] < 0, 1, 0)\n",
        "results_df_low_entropy = results_df[results_df['ent_coef'] < 0.1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Success Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create 3 figures for the combination of all 3 hyperparameters, learning rate, entropy, and gamma, and I want them stacked with one figure per row and 3 rows to compare the failure distribution of each hyperparameter combination \n",
        "# gamma x learning rate, gamma x entropy, learning rate x entropy\n",
        "# first create the subplot that each figure will go into with 1 column and 3 rows\n",
        "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
        "# create the first subplot with gamma x learning rate and status of ok being green and fail being red\n",
        "custom_palette = sns.color_palette([\"green\", \"red\"])\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='learning_rate', hue='status', ax=axs[0], palette=custom_palette)\n",
        "axs[0].set_title('Gamma x Learning Rate')\n",
        "# create the second subplot with gamma x entropy and status of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='ent_coef', hue='status', ax=axs[1], palette=custom_palette)\n",
        "axs[1].set_title('Gamma x Entropy')\n",
        "# create the third subplot with learning rate x entropy and status of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='learning_rate', y='ent_coef', hue='status', ax=axs[2], palette=custom_palette)\n",
        "axs[2].set_title('Learning Rate x Entropy')\n",
        "# add some space between each figure\n",
        "plt.tight_layout()\n",
        "# plot the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Binary Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create 3 figures for the combination of all 3 hyperparameters, learning rate, entropy, and gamma, and I want them stacked with one figure per row and 3 rows to compare the failure distribution of each hyperparameter combination \n",
        "# gamma x learning rate, gamma x entropy, learning rate x entropy\n",
        "# first create the subplot that each figure will go into with 1 column and 3 rows\n",
        "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
        "# create the first subplot with gamma x learning rate and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='learning_rate', hue='loss_binary', ax=axs[0], palette=custom_palette)\n",
        "axs[0].set_title('Gamma x Learning Rate')\n",
        "# create the second subplot with gamma x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='ent_coef', hue='loss_binary', ax=axs[1], palette=custom_palette)\n",
        "axs[1].set_title('Gamma x Entropy')\n",
        "# create the third subplot with learning rate x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='learning_rate', y='ent_coef', hue='loss_binary', ax=axs[2], palette=custom_palette)\n",
        "axs[2].set_title('Learning Rate x Entropy')\n",
        "# add some space between each figure\n",
        "plt.tight_layout()\n",
        "# plot the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_low_entropy_negative = results_df_low_entropy[results_df_low_entropy['loss'] < 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Binary Loss Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create 3 figures for the combination of all 3 hyperparameters, learning rate, entropy, and gamma, and I want them stacked with one figure per row and 3 rows to compare the failure distribution of each hyperparameter combination \n",
        "# gamma x learning rate, gamma x entropy, learning rate x entropy\n",
        "# first create the subplot that each figure will go into with 1 column and 3 rows\n",
        "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
        "# create the first subplot with gamma x learning rate and loss of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy_negative, x='gamma', y='learning_rate', hue='loss', ax=axs[0])\n",
        "axs[0].set_title('Gamma x Learning Rate')\n",
        "# create the second subplot with gamma x entropy and loss of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy_negative, x='gamma', y='ent_coef', hue='loss', ax=axs[1])\n",
        "axs[1].set_title('Gamma x Entropy')\n",
        "# create the third subplot with learning rate x entropy and loss of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy_negative, x='learning_rate', y='ent_coef', hue='loss', ax=axs[2])\n",
        "axs[2].set_title('Learning Rate x Entropy')\n",
        "# add some space between each figure\n",
        "plt.tight_layout()\n",
        "# plot the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters vs loss plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create 3 figures for the combination of all 3 hyperparameters, learning rate, entropy, and gamma, and I want them stacked with one figure per row and 3 rows to compare the failure distribution of each hyperparameter combination \n",
        "# gamma x learning rate, gamma x entropy, learning rate x entropy\n",
        "# first create the subplot that each figure will go into with 1 column and 3 rows\n",
        "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
        "# create the first subplot with gamma x learning rate and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='gamma', y='loss', hue='loss_binary', ax=axs[0], palette=custom_palette)\n",
        "axs[0].set_title('Gamma x Loss')\n",
        "# create the second subplot with gamma x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='ent_coef', y='loss', hue='loss_binary', ax=axs[1], palette=custom_palette)\n",
        "axs[1].set_title('Entropy x Loss')\n",
        "# create the third subplot with learning rate x entropy and loss_binary of ok being green and fail being red\n",
        "sns.scatterplot(data=results_df_low_entropy, x='learning_rate', y='loss', hue='loss_binary', ax=axs[2], palette=custom_palette)\n",
        "axs[2].set_title('Learning Rate x Loss')\n",
        "# add some space between each figure\n",
        "plt.tight_layout()\n",
        "# plot the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize the parameters that cause failures in the objective function\n",
        "\n",
        "# create a graph that has learning rate on the x-axis and ent_coef on the y-axis, \n",
        "# then the color of the points is whether the status is ok or fail, green for ok and red for fail\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df['learning_rate'], results_df['ent_coef'], \n",
        "                     c=results_df['status'].apply(lambda x: 'green' if x == 'ok' else 'red'))\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Entropy Coefficient')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "# y lim to 0.2\n",
        "plt.ylim(0, 0.1)\n",
        "# x lim to 0.05\n",
        "# plt.xlim(0, 0.05)\n",
        "# plt.legend(handles=scatter.legend_elements()[0], labels=['OK', 'Fail'])\n",
        "# increase the plot size\n",
        "fig.set_size_inches(20, 20)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Hyperparameters vs Next Week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# only successes \n",
        "results_df_success = results_df[results_df['status'] == 'ok']\n",
        "results_df_success_negative = results_df_success[results_df_success['loss'] < 0]\n",
        "# sort values from least to greatest loss\n",
        "results_df_success_negative_sorted = results_df_success_negative.sort_values(by='loss', ascending=True)\n",
        "results_df_success_negative_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_train = MtEnv(\n",
        "    original_simulator=sim_train,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(symbols[1]['EURUSD'].loc[:(max_friday - pd.DateOffset(days=7)), :].index),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 2024  # random seed\n",
        "total_num_episodes = 10\n",
        "\n",
        "# print (\"env_name                 :\", env_name)\n",
        "print (\"seed                     :\", seed)\n",
        "\n",
        "# INIT matplotlib\n",
        "plot_settings = {}\n",
        "plot_data = {'x': [i for i in range(1, total_num_episodes + 1)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective_testing(params):\n",
        "    learning_timesteps = 50 #params['learning_timesteps']\n",
        "    ent_coef = params['ent_coef']\n",
        "    gamma = params['gamma'] #0.99 #\n",
        "    learning_rate = params['learning_rate']#0.0003#\n",
        "\n",
        "    total_learning_timesteps = learning_timesteps * 1000\n",
        "    step_key = f'{learning_timesteps}K'\n",
        "    policy_dict = PPO.policy_aliases\n",
        "    policy = policy_dict.get('MultiInputPolicy')\n",
        "    class_name = type(PPO).__qualname__\n",
        "    plot_key = f'{class_name}_rewards_'+step_key\n",
        "    try:\n",
        "        rewards, orders = train_val_model(PPO, policy, env_train, env_testing, seed, step_key,  \n",
        "                                                    learning_rate, gamma, ent_coef, total_learning_timesteps)\n",
        "    except:\n",
        "        print(f'''there was an error with those parameters: timesteps: {learning_timesteps}, \\n\n",
        "              ent_coef: {ent_coef}, gamma: {gamma}, learning_rate: {learning_rate}''')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    # timesteps_models_dict[step_key] = models_dict\n",
        "    min_rewards, avg_rewards, max_rewards, = print_stats(rewards, 'Reward')\n",
        "    print_stats(orders, 'Orders')\n",
        "    label = f'Avg. {avg_rewards:>7.2f} : {class_name} - {step_key}'\n",
        "    plot_data[plot_key] = rewards\n",
        "    plot_settings[plot_key] = {'label': label}\n",
        "    params['avg_orders'] = np.mean(orders)       \n",
        "\n",
        "    return {'loss': -avg_rewards, 'status': STATUS_OK, 'eval_time': time.time(), 'parameters': params}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results for best models from hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# how does the model perform on the following week\n",
        "hyperparameter_tune_weekly_result_next_week = []\n",
        "results_df_success_negative_no_zero = results_df_success_negative_sorted[results_df_success_negative_sorted['iteration'] == 7]\n",
        "for row in range(0, len(results_df_success_negative_no_zero)):\n",
        "    print(f\"{'-'*40} loss: {round(results_df_success_negative_no_zero.iloc[row, 0], 2)} {'-'*40}\")\n",
        "    i = results_df_success_negative_no_zero.iloc[row, 3] - 7    \n",
        "    parameters = {\n",
        "        'learning_rate': results_df_success_negative_no_zero.iloc[row, 4],\n",
        "        'ent_coef': results_df_success_negative_no_zero.iloc[row, 5],\n",
        "        'gamma': results_df_success_negative_no_zero.iloc[row, 6]\n",
        "    }\n",
        "    result = objective_testing(parameters)\n",
        "    hyperparameter_tune_weekly_result_next_week.append(result)\n",
        "    # print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_hyperparameter_tuned = pd.DataFrame()\n",
        "for idx, result in enumerate(hyperparameter_tune_weekly_result_next_week):\n",
        "    result['learning_rate'] = result['parameters']['learning_rate']\n",
        "    result['ent_coef'] = result['parameters']['ent_coef']\n",
        "    result['gamma'] = result['parameters']['gamma']\n",
        "    result['orders'] = result['parameters']['orders']\n",
        "    del result['parameters']\n",
        "    new_row = pd.DataFrame(result, index=[idx])\n",
        "    results_df_hyperparameter_tuned = pd.concat([results_df_hyperparameter_tuned, new_row], axis=0)\n",
        "results_df_hyperparameter_tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_hyperparameter_tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr = results_df_success_negative_sorted.iloc[1,4]\n",
        "entropy = results_df_success_negative_sorted.iloc[1,5]\n",
        "gamma = results_df_success_negative_sorted.iloc[1,6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 2024\n",
        "rewards, orders = train_val_model(PPO, 'MultiInputPolicy', env_train, env_testing, seed, '50K',  \n",
        "                                                    lr, gamma, entropy, 250_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming env_orders is your DataFrame and 'column_name' is the name of the column you want to normalize\n",
        "orders = env_testing.render()['orders']\n",
        "# Calculate the mean of the column\n",
        "mean_value = orders['Volume'].mean()\n",
        "\n",
        "# Normalize the column to have a mean of 1\n",
        "orders['Volume_normalized'] = orders['Volume'] / mean_value\n",
        "orders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_profit_calculation(orders, 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders['Volume_normalized'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_testing.render('advanced_figure', time_format=\"%Y-%m-%d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make a sound when the code is done\n",
        "import winsound\n",
        "frequency = 2500  # Set Frequency To 2500 Hertz\n",
        "duration = 2000  # Set Duration To 1000 ms == 1 second\n",
        "winsound.Beep(frequency, duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate a 3d plot of the learning rate, ent_coef, and loss\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "x = results_df_success_negative['learning_rate']\n",
        "y = results_df_success_negative['loss']\n",
        "z = results_df_success_negative['ent_coef']\n",
        "\n",
        "ax.scatter(x, y, z, c=z, cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_zlabel('Entropy Coefficient')\n",
        "\n",
        "# increase the plot size\n",
        "fig.set_size_inches(20, 20)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate a 3d plot of the learning rate, ent_coef, and loss\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "results_df_success_negative_low_entropy = results_df_success_negative[results_df_success_negative['ent_coef'] <= 0.2]\n",
        "\n",
        "x = results_df_success_negative_low_entropy['learning_rate']\n",
        "y = results_df_success_negative_low_entropy['loss']\n",
        "z = results_df_success_negative_low_entropy['ent_coef']\n",
        "\n",
        "ax.scatter(x, y, z, c=z, cmap=cm.coolwarm)\n",
        "\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_zlabel('Entropy Coefficient')\n",
        "\n",
        "\n",
        "\n",
        "# increase the plot size\n",
        "fig.set_size_inches(20, 20)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize the parameters that cause failures in the objective function\n",
        "\n",
        "# create a graph that has learning rate on the x-axis and ent_coef on the y-axis, \n",
        "# then the color of the points is whether the status is ok or fail, green for ok and red for fail\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df_success['learning_rate'], results_df_success['ent_coef'], \n",
        "                     c=results_df_success['loss'].apply(lambda x: 'green' if x < 0 else 'red'))\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Entropy Coefficient')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "# plt.legend(handles=scatter.legend_elements()[0], labels=['OK', 'Fail'])\n",
        "plt.ylim(0, 0.1)\n",
        "# increase the plot size\n",
        "fig.set_size_inches(20, 20)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# create a heatmap with learning rate on the x-axis and ent_coef on the y-axis and the color is the loss\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df_success['learning_rate'], results_df_success['ent_coef'], \n",
        "                     c=results_df_success['loss'], cmap='viridis')\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Entropy Coefficient')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "plt.ylim(0, 0.1)\n",
        "# increase the plot size\n",
        "fig.set_size_inches(25, 15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_success_low_entropy = results_df_success[results_df_success['ent_coef'] <= 0.2]\n",
        "# create a heatmap with learning rate on the x-axis and ent_coef on the y-axis and the color is the loss\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df_success_low_entropy['learning_rate'], results_df_success_low_entropy['loss'], \n",
        "                     c=results_df_success_low_entropy['ent_coef'], cmap=cm.coolwarm)\n",
        "ax.set_xlabel('Learning Rate')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "# plt.ylim(-50_000, 50_000)\n",
        "# increase the plot size\n",
        "fig.set_size_inches(25, 15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df_success_low_entropy = results_df_success[results_df_success['ent_coef'] <= 0.2]\n",
        "# create a heatmap with learning rate on the x-axis and ent_coef on the y-axis and the color is the loss\n",
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(results_df_success_low_entropy['ent_coef'], results_df_success_low_entropy['loss'], \n",
        "                     c=results_df_success_low_entropy['learning_rate'], cmap=cm.coolwarm)\n",
        "ax.set_xlabel('Entropy Coefficient')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Hyperparameter Optimization')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "# plt.ylim(-50_000, 50_000)\n",
        "# increase the plot size\n",
        "fig.set_size_inches(25, 15)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # create a dataframe of the rewards\n",
        "# rewards_df = pd.DataFrame({'rewards': rewards})\n",
        "# # plot the rewards\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.lineplot(data=rewards_df)\n",
        "# plt.title('Rewards')\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Reward')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # import the models from /models folder\n",
        "# import os\n",
        "# import glob\n",
        "# # get the list of models\n",
        "# model_list = glob.glob('models_4_17_24/*.pkl')\n",
        "# # separate the strings of each model name on _ and get the last element of the string if the string of the model doesn't include 'initial' or 'updated'\n",
        "# model_list_episode_nbr = [model.split('_')[-1] for model in model_list if 'initial' not in model and 'updated' not in model]\n",
        "# model_list_episode_nbr = [int(model_name.split('.')[0]) for model_name in model_list_episode_nbr]\n",
        "# max_episode = max(model_list_episode_nbr)\n",
        "# # test the last set of 10 episodes\n",
        "# init_episode = ((int(max_episode)/10) - 10)*10\n",
        "# # print(max_episode, init_episode)\n",
        "# models = []\n",
        "# # test the last set of 10 episodes from init_episode to max_episode\n",
        "# for nbr in range(int(init_episode), int(max_episode)+10, 10):\n",
        "#     # set up the appropriate time_points for each of the models in the list\n",
        "#     env_train.time_points = list(symbols[1]['EURUSD'].iloc[-int(training_length):-(int(testing_length)-int(nbr)), :].index)# make this -nbr not +nbr next time\n",
        "#     obs_train, info_train = env_train.reset(seed=2024)\n",
        "#     # find the model name that contains the nbr\n",
        "#     model_name = [model for model in model_list if str(nbr) in model][0]\n",
        "#     print(model_name)\n",
        "#     # load the models into a list\n",
        "#     models.append(PPO.load(model_name, env=env_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sim_testing = gym_mtsim.MtSimulator(\n",
        "#     unit='USD',\n",
        "#     balance=200000.,\n",
        "#     leverage=100.,\n",
        "#     stop_out_level=0.2,\n",
        "#     hedge=True,\n",
        "#     symbols_filename=FOREX_DATA_PATH\n",
        "# )\n",
        "\n",
        "# env_testing = MtEnv(\n",
        "#     original_simulator=sim_testing,\n",
        "#     trading_symbols=['EURUSD'],\n",
        "#     window_size = window_size_param,\n",
        "#     time_points=list(testing_index_slice),\n",
        "#     hold_threshold=0.1,\n",
        "#     close_threshold=0.1,\n",
        "#     fee=lambda symbol: {\n",
        "#         # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "#         'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "#         # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "#     }[symbol],\n",
        "#     symbol_max_orders=2,\n",
        "#     multiprocessing_processes=2\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_ppo = PPO.load(f'models_4_17_24\\model_25K_5.pkl', env=env_train)\n",
        "\n",
        "# obs_test, info_test = env_testing.reset(seed=2024)\n",
        "# done_test = False\n",
        "# while not done_test:\n",
        "#     action, _states = model_ppo.predict(obs_test)\n",
        "#     obs_test, reward_test, terminated_test, truncated_test, info_test = env_testing.step(action)\n",
        "#     done_test = terminated_test or truncated_test\n",
        "#     # total_reward += reward_test\n",
        "#     if done_test:\n",
        "#         break\n",
        "# try:\n",
        "#     order_len = len(env_testing.render()['orders'])\n",
        "# except:\n",
        "#     order_len = 0\n",
        "# # print(f\"Episode: {episode}, Reward: {total_reward:.3f}, # orders: {order_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # if model_dict is still a thing\n",
        "# for timestep in timesteps_models_dict.keys():\n",
        "#     models_dict = timesteps_models_dict[timestep]\n",
        "\n",
        "#     for nbr in range(0, 10):\n",
        "#         msg = f\"{'-'*8} Testing Model {nbr} with {timestep} training timesteps {'-'*8}\"\n",
        "#         print(f\"\"\"{msg}\\n{'-'*len(msg)}\"\"\")\n",
        "#         reward_across_episodes = []\n",
        "#         rewards_dict = {}\n",
        "#         model_results_dict = {}\n",
        "#         for episode in range(0, 10):   \n",
        "#             total_reward = 0\n",
        "#             done_test = False\n",
        "#             model_ppo = models_dict[f'model_{nbr}']\n",
        "\n",
        "#             obs_test, info_test = env_train.reset(seed=2024)\n",
        "#             while not done_test:\n",
        "#                 action, _states = model_ppo.predict(obs_test)\n",
        "#                 obs_test, reward_test, terminated_test, truncated_test, info_test = env_train.step(action)\n",
        "#                 done_test = terminated_test or truncated_test\n",
        "#                 total_reward += reward_test\n",
        "#                 if done_test:\n",
        "#                     break\n",
        "#             reward_across_episodes.append(total_reward)\n",
        "#             try:\n",
        "#                 order_len = len(env_train.render()['orders'])\n",
        "#             except:\n",
        "#                 order_len = 0\n",
        "#             print(f\"Episode: {episode}, Reward: {total_reward:.3f}, # orders: {order_len}\")\n",
        "#         print_stats(reward_across_episodes)\n",
        "#         model_results_dict[f'model_{nbr}_{timestep}'] = reward_across_episodes\n",
        "# model_results_df = pd.DataFrame(model_results_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # plot the rewards for each model over episodes\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.lineplot(data=model_results_df)\n",
        "# plt.title('Rewards')\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Reward')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # if model_dict is still a thing\n",
        "# for timestep in ['25K']:\n",
        "#     for nbr in tqdm(range(1, 10)):\n",
        "#         msg = f\"{'-'*8} Testing Model {nbr} with {timestep} training timesteps {'-'*8}\"\n",
        "#         print(f\"\"\"{msg}\\n{'-'*len(msg)}\"\"\")\n",
        "#         reward_across_episodes = []\n",
        "#         number_of_orders_across_episodes = []\n",
        "#         rewards_dict = {}\n",
        "#         model_results_dict = {}\n",
        "#         for episode in tqdm(range(0, 10)):   \n",
        "#             total_reward = 0\n",
        "#             done_test = False\n",
        "#             model_ppo = PPO.load(f'models_4_17_24\\model_{timestep}_{nbr}.pkl', env=env_train)\n",
        "\n",
        "#             obs_test, info_test = env_testing.reset(seed=2024)\n",
        "#             while not done_test:\n",
        "#                 action, _states = model_ppo.predict(obs_test)\n",
        "#                 obs_test, reward_test, terminated_test, truncated_test, info_test = env_testing.step(action)\n",
        "#                 done_test = terminated_test or truncated_test\n",
        "#                 total_reward += reward_test\n",
        "#                 if done_test:\n",
        "#                     break\n",
        "#             reward_across_episodes.append(total_reward)\n",
        "#             try:\n",
        "#                 order_len = len(env_testing.render()['orders'])\n",
        "#             except:\n",
        "#                 order_len = 0\n",
        "#             number_of_orders_across_episodes.append(order_len)\n",
        "#             print(f\"Episode: {episode}, Reward: {total_reward:.3f}, # orders: {order_len}\")\n",
        "#         print_stats(reward_across_episodes, 'Reward')\n",
        "#         print_stats(number_of_orders_across_episodes, 'Orders')\n",
        "#         model_results_dict[f'model_{nbr}_{timestep}'] = reward_across_episodes\n",
        "# model_results_df = pd.DataFrame(model_results_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # plot the rewards for each model over episodes\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# sns.lineplot(data=model_results_df)\n",
        "# plt.title('Rewards')\n",
        "# plt.xlabel('Episode')\n",
        "# plt.ylabel('Reward')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # if the cluster has been restarted\n",
        "# for nbr, time_points_idx in zip(range(0, 10), range(0, 500, 50)):\n",
        "#     # model_ppo.learn(total_timesteps=25000, callback=ProgressBarCallback(100))\n",
        "#     env_train = MtEnv(\n",
        "#         original_simulator=sim_train,\n",
        "#         trading_symbols=['EURUSD'],\n",
        "#         window_size = window_size_param,\n",
        "#         time_points=list(symbols[1]['EURUSD'].iloc[-int(training_length):-(int(testing_length)-int(time_points_idx)), :].index),\n",
        "#         hold_threshold=0.5,\n",
        "#         close_threshold=0.5,\n",
        "#         fee=lambda symbol: {\n",
        "#             # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "#             'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "#             # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "#         }[symbol],\n",
        "#         symbol_max_orders=2,\n",
        "#         multiprocessing_processes=2\n",
        "#     )\n",
        "#     # obs_train, info_train = env_train.reset(seed=2024)\n",
        "#     total_reward = 0\n",
        "#     done_test = False\n",
        "#     model_ppo = PPO.load(f'models\\model_{nbr}.pkl', env=env_train)\n",
        "\n",
        "#     env_testing = MtEnv(\n",
        "#         original_simulator=sim_testing,\n",
        "#         trading_symbols=['EURUSD'],\n",
        "#         window_size = window_size_param,\n",
        "#         # time_points=list(testing_index_slice),\n",
        "#         hold_threshold=0.5,\n",
        "#         close_threshold=0.5,\n",
        "#         fee=lambda symbol: {\n",
        "#             # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "#             'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "#             # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "#         }[symbol],\n",
        "#         symbol_max_orders=2,\n",
        "#         multiprocessing_processes=2\n",
        "#     )\n",
        "#     obs_test, info_test = env_testing.reset(seed=2024)\n",
        "#     while not done_test:\n",
        "#         action, _states = model_ppo.predict(obs_test)\n",
        "#         obs_test, reward_test, terminated_test, truncated_test, info_test = env_testing.step(action)\n",
        "#         done_test = terminated_test or truncated_test\n",
        "#         total_reward += reward_test\n",
        "#         if done_test:\n",
        "#             break\n",
        "#     state = env_testing.render()\n",
        "\n",
        "#     print(\n",
        "#         f\"balance: {state['balance']}, equity: {state['equity']}, margin: {state['margin']}\\n\"\n",
        "#         f\"free_margin: {state['free_margin']}, margin_level: {state['margin_level']}\\n\"\n",
        "\n",
        "#     )\n",
        "#     # print(state['orders'].Profit.sum())\n",
        "#     if len(state['orders']) > 0:\n",
        "#         print(state['orders'].Profit.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max_episode = 120\n",
        "# for model_nbr in range(0, int(max_episode)+10, 10):\n",
        "#     print(f'Model: {model_nbr}')\n",
        "#     over_episodes_rewards = []\n",
        "#     over_episodes_balance = []\n",
        "#     obs_training, info_training = env_train.reset(seed=2024)\n",
        "#     for episode in range(0, 10):\n",
        "#         obs_test, info_test = env_testing.reset(seed=2024)\n",
        "        \n",
        "#         # model_ppo.learn(total_timesteps=25000, callback=ProgressBarCallback(100))\n",
        "\n",
        "#         total_reward = 0\n",
        "#         done_test = False\n",
        "#         env_train.time_points = list(symbols[1]['EURUSD'].iloc[-int(training_length):-(int(testing_length)-int(model_nbr)), :].index)\n",
        "#         obs_training, info_training = env_train.reset(seed=2024)\n",
        "#         model_ppo = PPO.load(f'models\\model_{model_nbr}.pkl', env=env_train)\n",
        "\n",
        "#         while not done_test:\n",
        "#             action, _states = model_ppo.predict(obs_test)\n",
        "#             obs_test, reward_test, terminated_test, truncated_test, info_test = env_testing.step(action)\n",
        "#             done_test = terminated_test or truncated_test\n",
        "\n",
        "#             total_reward += reward_test\n",
        "#             if done_test:\n",
        "#                 break\n",
        "#         over_episodes_balance.append(info_test['balance'])\n",
        "#         over_episodes_rewards.append(total_reward)\n",
        "#         print(f'Episode: {episode}, Reward: {total_reward:.3f}, Balance: {info_test[\"balance\"]:.3f}')\n",
        "#     print_stats(over_episodes_rewards)\n",
        "#     print_stats(over_episodes_balance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = pd.DataFrame(plot_data)\n",
        "\n",
        "# sns.set_style('whitegrid')\n",
        "# plt.figure(figsize=(8, 6))\n",
        "\n",
        "# for key in plot_data:\n",
        "#     if key == 'x':\n",
        "#         continue\n",
        "#     label = plot_settings[key]['label']\n",
        "#     line = plt.plot('x', key, data=data, linewidth=1, label=label)\n",
        "\n",
        "# plt.xlabel('episode')\n",
        "# plt.ylabel('reward')\n",
        "# plt.title('Random vs. SB3 Agents')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "p3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
