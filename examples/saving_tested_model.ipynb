{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gymnasium as gym\n",
        "import gym_mtsim\n",
        "sys.path.append(\"C:/Users/WilliamFetzner/Documents/Trading/\")\n",
        "from gym_mtsim_forked.gym_mtsim.data import FOREX_DATA_PATH_TRAIN, FOREX_DATA_PATH_TEST, FOREX_DATA_PATH\n",
        "from gym_mtsim import OrderType, Timeframe, MtEnv, MtSimulator\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, STATUS_FAIL\n",
        "from stable_baselines3 import A2C, PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "import time\n",
        "import torch\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# unpack the pickle file and load the data that is in symbols_forex.pkl\n",
        "with open('C:/Users/WilliamFetzner/Documents/Trading/gym_mtsim_forked/gym_mtsim/data/symbols_forex.pkl', 'rb') as f:\n",
        "    symbols = pickle.load(f)\n",
        "# convert symbols to a pd.dataframe\n",
        "# symbols[1]['EURUSD']\n",
        "split = int(len(symbols[1]['EURUSD']) * 0.80)\n",
        "validation_split = int(len(symbols[1]['EURUSD']) * 0.90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the 2 weeks of the symbols[1]['EURUSD'] dataframe by first finding the max date\n",
        "# then subtracting 14 days from that date\n",
        "symbols[1]['EURUSD'].index = pd.to_datetime(symbols[1]['EURUSD'].index)\n",
        "max_date = symbols[1]['EURUSD'].index.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find the previous friday before max_date\n",
        "# what is the day of the week of the max_date\n",
        "max_day_of_week = max_date.dayofweek\n",
        "# subtract the day of the week from the max_date to get the previous friday\n",
        "max_friday = max_date - pd.DateOffset(days=max_day_of_week+2)\n",
        "two_weeks = max_friday - pd.DateOffset(days=14)\n",
        "one_week = max_friday - pd.DateOffset(days=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_index_slice = symbols[1]['EURUSD'].loc[:one_week, :].index\n",
        "validation_index_slice = symbols[1]['EURUSD'].loc[one_week:max_friday, :].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatetimeIndex(['2024-04-08 00:00:00+00:00', '2024-04-08 01:00:00+00:00',\n",
              "               '2024-04-08 02:00:00+00:00', '2024-04-08 03:00:00+00:00',\n",
              "               '2024-04-08 04:00:00+00:00', '2024-04-08 05:00:00+00:00',\n",
              "               '2024-04-08 06:00:00+00:00', '2024-04-08 07:00:00+00:00',\n",
              "               '2024-04-08 08:00:00+00:00', '2024-04-08 09:00:00+00:00',\n",
              "               ...\n",
              "               '2024-04-12 14:00:00+00:00', '2024-04-12 15:00:00+00:00',\n",
              "               '2024-04-12 16:00:00+00:00', '2024-04-12 17:00:00+00:00',\n",
              "               '2024-04-12 18:00:00+00:00', '2024-04-12 19:00:00+00:00',\n",
              "               '2024-04-12 20:00:00+00:00', '2024-04-12 21:00:00+00:00',\n",
              "               '2024-04-12 22:00:00+00:00', '2024-04-12 23:00:00+00:00'],\n",
              "              dtype='datetime64[ns, UTC]', name='Time', length=120, freq=None)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_index_slice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_train = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_train = MtEnv(\n",
        "    original_simulator=sim_train,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(training_index_slice),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "sim_validation = gym_mtsim.MtSimulator(\n",
        "    unit='USD',\n",
        "    balance=200000.,\n",
        "    leverage=100.,\n",
        "    stop_out_level=0.2,\n",
        "    hedge=True,\n",
        "    symbols_filename=FOREX_DATA_PATH\n",
        ")\n",
        "\n",
        "env_validation = MtEnv(\n",
        "    original_simulator=sim_validation,\n",
        "    trading_symbols=['EURUSD'],\n",
        "    window_size = 10,\n",
        "    time_points=list(validation_index_slice),\n",
        "    hold_threshold=0.5,\n",
        "    close_threshold=0.5,\n",
        "    fee=lambda symbol: {\n",
        "        # 'GBPCAD': max(0., np.random.normal(0.0007, 0.00005)),\n",
        "        'EURUSD': max(0., np.random.normal(0.0001, 0.00003))\n",
        "        # 'USDJPY': max(0., np.random.normal(0.02, 0.003)),\n",
        "    }[symbol],\n",
        "    symbol_max_orders=2,\n",
        "    multiprocessing_processes=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(reward_over_episodes, printing_name):\n",
        "    \"\"\"  Print Reward  \"\"\"\n",
        "\n",
        "    avg_rewards = np.mean(reward_over_episodes)\n",
        "    min_rewards = np.min(reward_over_episodes)\n",
        "    max_rewards = np.max(reward_over_episodes)\n",
        "\n",
        "    print (f'Min. {printing_name}          : {min_rewards:>10.3f}')\n",
        "    print (f'Avg. {printing_name}          : {avg_rewards:>10.3f}')\n",
        "    print (f'Max. {printing_name}          : {max_rewards:>10.3f}')\n",
        "\n",
        "    return min_rewards, avg_rewards, max_rewards\n",
        "\n",
        "def my_profit_calculation(env_orders, stop_loss):\n",
        "        # env_orders = env_testing.render()['orders']\n",
        "        # stop_loss = 0.001\n",
        "\n",
        "        # normalize the Volume with to have a mean of 1\n",
        "        mean_value = env_orders['Volume'].mean()\n",
        "\n",
        "        # Normalize the column to have a mean of 1\n",
        "        env_orders.loc[:, 'Volume_normalized'] = round((env_orders['Volume'] / mean_value), 2)\n",
        "        # add a column for when the difference between the Entry Price and the Exit Price is greater than stop_loss\n",
        "        env_orders.loc[:, 'stoploss_hit'] = np.where((env_orders['Type'].str.strip() == 'Buy') &\n",
        "                                                        ((env_orders['Entry Price'] - env_orders['Exit Price']) > stop_loss),\n",
        "                                                        1, np.where((env_orders['Type'].str.strip() == 'Sell') &\n",
        "                                                                        ((env_orders['Exit Price'] - env_orders['Entry Price']) > stop_loss),\n",
        "                                                                        1, 0))\n",
        "        env_orders.loc[:, 'Exit Price'] = np.where((env_orders['Type'].str.strip() == 'Buy') & (env_orders['stoploss_hit'] == 1),\n",
        "                                                        env_orders['Entry Price'] - stop_loss,\n",
        "                                                        np.where((env_orders['Type'].str.strip() == 'Sell') & (env_orders['stoploss_hit'] == 1),\n",
        "                                                                env_orders['Entry Price'] + stop_loss, env_orders['Exit Price']))\n",
        "        env_orders.loc[:, 'Profit'] = np.where((env_orders['Type'].str.strip() == 'Buy'),\n",
        "                                                        ((env_orders['Exit Price'] - (env_orders['Fee']/2)) - \n",
        "                                                        (env_orders['Entry Price'] + (env_orders['Fee']/2)))\n",
        "                                                                * 100_000 * env_orders['Volume_normalized'], \n",
        "                                                        np.where((env_orders['Type'].str.strip() == 'Sell'),\n",
        "                                                                ((env_orders['Entry Price'] - (env_orders['Fee']/2)) - \n",
        "                                                                (env_orders['Exit Price'] + (env_orders['Fee']/2)))\n",
        "                                                                * 100_000 * env_orders['Volume_normalized'], np.nan))\n",
        "        total_reward = env_orders.loc[:, 'Profit'].sum()\n",
        "        # Calculate Gross Profit\n",
        "        gross_profit = env_orders.loc[env_orders['Profit'] > 0, 'Profit'].sum()\n",
        "\n",
        "        # Calculate Gross Loss\n",
        "        gross_loss = env_orders.loc[env_orders['Profit'] < 0, 'Profit'].abs().sum()\n",
        "\n",
        "        # Calculate Profit Factor\n",
        "        profit_factor = gross_profit / gross_loss if gross_loss != 0 else 0\n",
        "\n",
        "        profit_factor = profit_factor - 1\n",
        "\n",
        "        return profit_factor, total_reward, env_orders\n",
        "\n",
        "# ProgressBarCallback for model.learn()\n",
        "class ProgressBarCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq: int, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        self.progress_bar = tqdm(total=self.model._total_timesteps, desc=\"model.learn()\")\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            self.progress_bar.update(self.check_freq)\n",
        "        return True\n",
        "    \n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        self.progress_bar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAINING + TEST\n",
        "def train_val_model(model, model_policy, env_tr, env_val, seed, steps_str, lr, gamma_param, entropy, total_learning_timesteps=10_000):\n",
        "    \"\"\"\n",
        "    Trains and validates a model using the Proximal Policy Optimization (PPO) algorithm.\n",
        "\n",
        "    Args:\n",
        "        model (object): The model to be trained.\n",
        "        model_policy (object): The policy used by the model.\n",
        "        env_tr (object): The training environment.\n",
        "        env_val (object): The validation environment.\n",
        "        seed (int): The random seed for reproducibility.\n",
        "        steps_str (str): A string representing the number of steps.\n",
        "        window_size_param (int): The window size parameter.\n",
        "        lr (float): The learning rate.\n",
        "        gamma_param (float): The gamma parameter.\n",
        "        entropy (float): The entropy coefficient.\n",
        "        total_learning_timesteps (int, optional): The total number of learning timesteps. Defaults to 10,000.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the reward over validations, orders over validations, and the model dictionary.\n",
        "    \"\"\"\n",
        "    # reproduce training and test\n",
        "    print('-' * 80)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    #model_dict = {}\n",
        "    # env_tr.window_size = window_size_param\n",
        "    print(f'entropy: {entropy}, learning rate: {lr}, gamma: {gamma_param}')\n",
        "    # eval_callback = EvalCallback(env_tr, log_path='./logs/', eval_freq=1000)\n",
        "    model = PPO(model_policy, env_tr, verbose=0, ent_coef=entropy, learning_rate=lr)#, gamma=gamma_param, \n",
        "    obs_tr, info_tr = env_tr.reset(seed=seed)\n",
        "    # custom callback for 'progress_bar'\n",
        "    model.learn(total_timesteps=total_learning_timesteps)#, callback=ProgressBarCallback(100))\n",
        "\n",
        "    reward_over_validations = []\n",
        "    orders_over_validations = []\n",
        "    orders_over_validations_dfs = {}\n",
        "    profit_over_validations = []\n",
        "\n",
        "    for episode in range(0, 10):\n",
        "        obs_val, info_val = env_val.reset(seed=seed)\n",
        "\n",
        "        total_reward = 0\n",
        "        done_val = False\n",
        "\n",
        "        while not done_val:\n",
        "            action, _states = model.predict(obs_val)\n",
        "            obs_val, reward_val, terminated_val, truncated_val, info_val = env_val.step(action)\n",
        "            done_val = terminated_val or truncated_val\n",
        "\n",
        "            total_reward += reward_val\n",
        "            if done_val:\n",
        "                break\n",
        "        try:\n",
        "            orders_made_in_episode = env_val.render()['orders']\n",
        "            order_len = len(orders_made_in_episode)\n",
        "            total_reward, total_profit, orders_df = my_profit_calculation(orders_made_in_episode, 0.001)\n",
        "            orders_over_validations_dfs[f'{episode}'] = orders_df\n",
        "            \n",
        "        except:\n",
        "            print('There were not any orders produced by the model')\n",
        "            order_len = 0        \n",
        "\n",
        "        reward_over_validations.append(total_reward) \n",
        "        profit_over_validations.append(total_profit)   \n",
        "        orders_over_validations.append(order_len)  \n",
        "\n",
        "\n",
        "        # if episode % 1 == 0:\n",
        "        avg_reward = np.mean(reward_over_validations)\n",
        "        avg_orders = np.mean(orders_over_validations)\n",
        "        avg_profit = np.mean(profit_over_validations)\n",
        "        print(f'Episode: {episode}, Avg. Reward: {avg_reward:.3f}, # of orders: {avg_orders:.3f}, avg Profit: {avg_profit:.3f}')\n",
        "    model.save(f'best_hyperparameters/models_4_27_24/model_{steps_str}.pkl')\n",
        "    return reward_over_validations, orders_over_validations, orders_over_validations_dfs#, model_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seed                     : 2024\n"
          ]
        }
      ],
      "source": [
        "seed = 2024  # random seed\n",
        "total_num_episodes = 10\n",
        "\n",
        "# print (\"env_name                 :\", env_name)\n",
        "print (\"seed                     :\", seed)\n",
        "\n",
        "# INIT matplotlib\n",
        "plot_settings = {}\n",
        "plot_data = {'x': [i for i in range(1, total_num_episodes + 1)]}\n",
        "\n",
        "# learning_timesteps_list_in_K = [25]#, 50, 100]\n",
        "# learning_timesteps_list_in_K = [50, 250, 500]\n",
        "# learning_timesteps_list_in_K = [500, 1000, 3000, 5000]\n",
        "\n",
        "# RL Algorithms: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
        "\n",
        "timesteps_models_dict = {}\n",
        "def objective(params):\n",
        "    learning_timesteps = 100 #params['learning_timesteps']\n",
        "    ent_coef = params['ent_coef']\n",
        "    gamma = params['gamma'] #0.99 #\n",
        "    learning_rate = params['learning_rate']#0.0003#\n",
        "\n",
        "    if learning_rate > 0.08:\n",
        "        print(f'Learning rate too high: {learning_rate}')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    if ent_coef > 0.1:\n",
        "        print(f'Entropy too high: {ent_coef}')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "\n",
        "    total_learning_timesteps = learning_timesteps * 1000\n",
        "    step_key = f'{learning_timesteps}K'\n",
        "    policy_dict = PPO.policy_aliases\n",
        "    policy = policy_dict.get('MultiInputPolicy')\n",
        "    class_name = type(PPO).__qualname__\n",
        "    plot_key = f'{class_name}_rewards_'+step_key\n",
        "    try:\n",
        "        rewards, orders = train_val_model(PPO, policy, env_train, env_validation, seed, step_key,  \n",
        "                                                    learning_rate, gamma, ent_coef, total_learning_timesteps)\n",
        "    except:\n",
        "        print(f'''there was an error with those parameters: timesteps: {learning_timesteps}, \\n\n",
        "              ent_coef: {ent_coef}, gamma: {gamma}, learning_rate: {learning_rate}''')\n",
        "        return {'loss': None, 'status': STATUS_FAIL, 'eval_time': time.time(), 'parameters': params}\n",
        "    # timesteps_models_dict[step_key] = models_dict\n",
        "    min_rewards, avg_rewards, max_rewards, = print_stats(rewards, 'Reward')\n",
        "    print_stats(orders, 'Orders')\n",
        "    label = f'Avg. {avg_rewards:>7.2f} : {class_name} - {step_key}'\n",
        "    plot_data[plot_key] = rewards\n",
        "    plot_settings[plot_key] = {'label': label}\n",
        "    params['avg_orders'] = np.mean(orders)       \n",
        "\n",
        "    return {'loss': -avg_rewards, 'status': STATUS_OK, 'eval_time': time.time(), 'parameters': params} "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.01683527363887545"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparameters_current_week = pd.read_excel('best_hyperparameter_search_results.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "entropy: 0.01879460162445175, learning rate: 0.01683527363887545, gamma: 0.99\n",
            "Episode: 0, Avg. Reward: -0.166, # of orders: 42.000, avg Profit: -224.783\n",
            "Episode: 1, Avg. Reward: 0.745, # of orders: 41.000, avg Profit: 790.657\n",
            "Episode: 2, Avg. Reward: 0.264, # of orders: 33.667, avg Profit: 270.732\n",
            "Episode: 3, Avg. Reward: 0.072, # of orders: 29.250, avg Profit: 141.611\n",
            "Episode: 4, Avg. Reward: 0.038, # of orders: 29.000, avg Profit: 96.550\n",
            "Episode: 5, Avg. Reward: 0.283, # of orders: 31.500, avg Profit: 242.613\n",
            "Episode: 6, Avg. Reward: 0.408, # of orders: 32.714, avg Profit: 394.583\n",
            "Episode: 7, Avg. Reward: 0.308, # of orders: 31.250, avg Profit: 310.829\n",
            "Episode: 8, Avg. Reward: 0.220, # of orders: 31.444, avg Profit: 198.966\n",
            "Episode: 9, Avg. Reward: 0.233, # of orders: 31.700, avg Profit: 212.372\n"
          ]
        }
      ],
      "source": [
        "# # # check if it is working:\n",
        "parameters = {\n",
        "    # 'window_size': 10,\n",
        "    # 'learning_timesteps': 25,\n",
        "    'ent_coef': best_hyperparameters_current_week.loc[0,'ent_coef'],\n",
        "    'gamma': best_hyperparameters_current_week.loc[0,'gamma'],\n",
        "    'learning_rate': best_hyperparameters_current_week.loc[0,'learning_rate']\n",
        "}\n",
        "rewards_250, orders_250, orders_df_dict_250 = train_val_model(PPO, 'MultiInputPolicy', env_train, env_validation, seed, '250K',  \n",
        "                                            parameters['learning_rate'], parameters['gamma'], parameters['ent_coef'], 250_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "entropy: 0.01879460162445175, learning rate: 0.01683527363887545, gamma: 0.99\n",
            "Episode: 0, Avg. Reward: 1.530, # of orders: 60.000, avg Profit: 2588.582\n",
            "Episode: 1, Avg. Reward: 0.709, # of orders: 38.500, avg Profit: 1255.176\n",
            "Episode: 2, Avg. Reward: 0.578, # of orders: 38.667, avg Profit: 1014.547\n",
            "Episode: 3, Avg. Reward: 0.714, # of orders: 48.250, avg Profit: 1357.107\n",
            "Episode: 4, Avg. Reward: 0.390, # of orders: 40.600, avg Profit: 991.873\n",
            "Episode: 5, Avg. Reward: 0.219, # of orders: 36.500, avg Profit: 739.308\n",
            "Episode: 6, Avg. Reward: 0.047, # of orders: 32.429, avg Profit: 553.190\n",
            "Episode: 7, Avg. Reward: -0.027, # of orders: 30.750, avg Profit: 430.696\n",
            "Episode: 8, Avg. Reward: 0.120, # of orders: 30.333, avg Profit: 549.282\n",
            "Episode: 9, Avg. Reward: 0.156, # of orders: 32.800, avg Profit: 593.882\n"
          ]
        }
      ],
      "source": [
        "rewards_50, orders_50, orders_df_dict_50 = train_val_model(PPO, 'MultiInputPolicy', env_train, env_validation, seed, '250K',  \n",
        "                                            parameters['learning_rate'], parameters['gamma'], parameters['ent_coef'], 50_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2588.5820506547198"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "orders_df_dict['0'].Profit.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test which version of the model to take, does it make a difference whether I use model_0 or model_9\n",
        "obs_tr, info_tr = env_train.reset(seed=seed)\n",
        "obs_test, info_test = env_validation.reset(seed=seed)\n",
        "\n",
        "total_reward = 0\n",
        "done_test = False\n",
        "reward_over_tests = {}\n",
        "for training in ['', '250K_']:\n",
        "    for episode in range(500):\n",
        "        obs_test, info_test = env_validation.reset(seed=seed)\n",
        "        model_ppo = PPO.load(f'best_hyperparameters/models_4_27_24/model_{training}0.pkl', env_train)\n",
        "        done_test = False\n",
        "        rewards = []\n",
        "        while not done_test:\n",
        "            action, _states = model_ppo.predict(obs_test)\n",
        "            obs_test, reward_test, terminated_test, truncated_test, info_test = env_validation.step(action)\n",
        "            done_test = terminated_test or truncated_test\n",
        "            \n",
        "            total_reward += reward_test\n",
        "            if done_test:\n",
        "                break\n",
        "        try:\n",
        "            orders_made_in_episode_test = env_validation.render()['orders']\n",
        "            # orders_over_validations_dfs[f'{episode}'] = orders_made_in_episode_test\n",
        "            order_len = len(orders_made_in_episode_test)\n",
        "            total_reward, total_profit, orders_df = my_profit_calculation(orders_made_in_episode_test, 0.001)\n",
        "            rewards.append(total_reward)\n",
        "        except:\n",
        "            print('There were not any orders produced by the model')\n",
        "            order_len = 0\n",
        "    reward_over_tests[f'{training}_1'] = rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timesteps_test_df = pd.DataFrame(reward_over_tests, columns=['50K', '250K'])\n",
        "# plot the timesteps_test_df results with the index on the x axis and the values for the columns as the y axis and the column names as the color of the dots\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.scatterplot(data=timesteps_test_df)\n",
        "plt.title('50K vs 250K')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "p3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
